= Playwright Browser Agent System Design
:toc:
:toc-placement!:

== Overview
This document captures the end-to-end system design for the Playwright Browser Agent CLI demo. It maps each requirement from the PRD to modules and flows that a junior developer can implement.

== Tech Stack

- Python 3.13
- Typer (CLI framework)
- LangChain (agent orchestration)
- LiteLLM (LLM provider abstraction)
- Playwright-MCP (browser automation via Node subprocess, specifically `@playwright/mcp` NPM package)
- `langchain-mcp-adapters` (PyPI package to integrate LangChain with MCP)
- python-dotenv (env config)
- Poetry (packaging, via pyproject.toml)
- Optionally: Streamlit (future web UI), CrewAI/AutoGen (future multi-agent)
- OS: macOS, Linux, Windows WSL (with Python 3.13 and Node.js for MCP)

== 1. Project Layout
[source,text]
----
playwright-browser-agent/
├── main.py           # Typer entrypoint
├── cli.py            # Defines `chat` & `batch` commands and flags
├── config.py         # Loads .env, merges JSON, applies CLI overrides
├── agent.py          # Builds LangChain<>LiteLLM<>Playwright-MCP pipeline
├── prompts.py        # System-prompt templates
├── utils.py          # Helpers: timestamp, keypress, shutdown
└── pyproject.toml    # Dependencies + console_scripts pb-agent
----

== 2. Config Flow (config.py)

Load and merge configuration in the following precedence:

[source,python]
----
# 1. Load environment variables via python-dotenv
# 2. If --model-config JSON is provided: parse and merge into defaults
# 3. Override with explicit CLI flags: provider, model, mode, record
# 4. Fail fast if chosen PROVIDER_API_KEY is missing
----

== 3. CLI Definition (cli.py)

Use Typer to expose two commands: `chat` and `batch`.

=== chat
[source,python]
----
@app.command()
def chat(
  provider: str = Option(...),
  model: str = Option("gpt-3.5-turbo"),
  record: bool = Option(False),
):
    cfg = config.load(
        provider=provider,
        model=model,
        record=record
    )
    asyncio.run(agent.run_agent_chat_session(cfg))
----

=== batch
[source,python]
----
@app.command()
def batch(
  file: Path,
  provider: str = Option(...),  # same flags as chat
  model: str = Option(...),
  record: bool = Option(False),
):
    cfg = config.load(
        provider=provider,
        model=model,
        record=record
    )
    lines = [l for l in file.read_text().splitlines() if l.strip()]
    asyncio.run(agent.run_agent_batch_session(cfg, lines))
    utils.wait_for_keypress()
----

== 4. Agent Core (agent.py)

Instantiate a streaming chat agent using LangGraph, wired to the Playwright-MCP tool via `langchain-mcp-adapters`.

[source,python]
----
# Example imports (adjust based on actual structure)
from langgraph.prebuilt import create_react_agent
from langchain_community.chat_models import ChatLiteLLM
from langchain_mcp_adapters.client import MultiServerMCPClient # Import the MCP client
# Assume prompts.py and config.py are correctly set up
from .prompts import build_system_prompt # Use actual import

class Agent:
    # NOTE: Initialization and running methods likely need to be async
    # due to the async nature of MultiServerMCPClient and agent invocation.
    # Simplified here for design clarity.

    def __init__(self, cfg):
        self.cfg = cfg
        # 1. Initialize LLM
        self.llm = ChatLiteLLM(provider=cfg.provider, model=cfg.model, streaming=True)

        # 2. Setup MCP Client and get tools conceptually
        # The actual setup (including passing mode/headless to configure MCP server)
        # happens within _setup_agent_resources called by run_..._session
        llm, mcp_config, system_prompt = _setup_agent_resources(cfg)

        # Tools are loaded async within the run methods, simplified here
        tools = ["PlaceholderTool"] # Placeholder for conceptual design

        # 3. Prepare system message/prompt using the actual function
        # system_message is built inside _setup_agent_resources now, using build_system_prompt
        # system_message = prompts.build_system_prompt(
        #     mode=self.cfg.mode,
        #     headless=self.cfg.headless,
        #     record_screenshots=self.cfg.record
        # )

        # 4. Create the LangGraph agent graph
        self.graph = create_react_agent(
            self.llm,
            tools=tools,
            # System prompt is typically passed within the messages input dictionary
            # state_modifier=system_message # Pass prompt via state_modifier - Not typical for create_react_agent
        )

    # Remove _load_mcp_tools_sync and run_chat_loop placeholders
    # as the logic is now in async run_agent_*_session functions

    def send(self, user_str):
        # Simplified send using the LangGraph agent - Primarily for batch concept
        print(f"Processing: {user_str}")
        # Actual invocation would be async: await self.graph.ainvoke or astream_events
        # System prompt is passed in messages list in the actual implementation
        response = self.graph.invoke({"messages": [
            ("system", "Placeholder System Prompt"), # System prompt passed here
            ("user", user_str)
        ]})
        final_message = response.get("messages", [])[-1]
        print(f"RESPONSE: {getattr(final_message, 'content', '...')}")
----

== 5. System Prompt Templates (prompts.py)

Encapsulate LLM instructions; include screenshot hint only if `--record` is set.

[source,python]
----
def build_system_prompt(record_screenshots: bool = False) -> str:
    # Base template with placeholders
    base_template = """
    You are a web browsing agent...
    {headless_status_detail}
    ...
    Current Configuration:
    - Headless: {headless_status}
    """

    # Format the template with selected descriptions, instructions, and status

    if record_screenshots:
        prompt += "\n\n" + SCREENSHOT_INSTRUCTIONS

    return prompt.strip()
----

== 6. Packaging & Entry Point

- Add in `pyproject.toml`:

[source,toml]
----
[tool.poetry.scripts]
pb-agent = "main:app"
----

- `main.py` simply invokes Typer:

[source,python]
----
from cli import app

if __name__ == "__main__":
    app()
----

== 7. Lifecycle & Shutdown Helpers (utils.py)

- Register SIGINT/SIGTERM handler for graceful application shutdown.
  (Note: Management of the external Playwright MCP server process is outside this application's scope).
- Implement `wait_for_keypress()` for batch completion.

[source,python]
----
import signal

def wait_for_keypress():
    # cross-platform getch or input
    input("Done. Press any key to exit …")

def register_shutdown(handler):
    signal.signal(signal.SIGINT, handler)
    signal.signal(signal.SIGTERM, handler)
----

== 8. Flow Summary

1. **Startup**: `main` → `cli` → `config` → instantiate `Agent`
2. **Interactive**: prompt user → chain.stream → Playwright-MCP tool → print tokens
3. **Batch**: read lines → for each line call `send` → stream output live → keypress end
4. **Shutdown**: on exit or signal, kill MCP server & exit

--

All PRD requirements are covered in discrete modules. Junior devs can follow each file stub and fill in details using the provided code snippets.