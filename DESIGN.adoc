= Playwright Browser Agent System Design
:toc:
:toc-placement!:

== Overview
This document captures the end-to-end system design for the Playwright Browser Agent CLI demo. It maps each requirement from the PRD to modules and flows that a junior developer can implement.

== Tech Stack

- Python 3.13
- Typer (CLI framework)
- LangChain (agent orchestration)
- LiteLLM (LLM provider abstraction)
- Playwright-MCP (browser automation via Node subprocess, specifically `@playwright/mcp` NPM package)
- `langchain-mcp-adapters` (PyPI package to integrate LangChain with MCP)
- python-dotenv (env config)
- Poetry (packaging, via pyproject.toml)
- Optionally: Streamlit (future web UI), CrewAI/AutoGen (future multi-agent)
- OS: macOS, Linux, Windows WSL (with Python 3.13 and Node.js for MCP)

== 1. Project Layout
[source,text]
----
playwright-browser-agent/
├── main.py           # Typer entrypoint
├── cli.py            # Defines `chat` & `batch` commands and flags
├── config.py         # Loads .env, merges JSON, applies CLI overrides
├── agent.py          # Builds LangChain<>LiteLLM<>Playwright-MCP pipeline
├── prompts.py        # System-prompt templates
├── utils.py          # Helpers: timestamp, keypress, shutdown
└── pyproject.toml    # Dependencies + console_scripts pb-agent
----

== 2. Config Flow (config.py)

Load and merge configuration in the following precedence:

[source,python]
----
# 1. Load environment variables via python-dotenv
# 2. If --model-config JSON is provided: parse and merge into defaults
# 3. Override with explicit CLI flags: provider, model, mode, record
# 4. Fail fast if chosen PROVIDER_API_KEY is missing
----

== 3. CLI Definition (cli.py)

Use Typer to expose two commands: `chat` and `batch`.

=== chat
[source,python]
----
@app.command()
def chat(
  provider: str = Option(...),
  model: str = Option("gpt-3.5-turbo"),
  mode: MCPMode = Option(MCPMode.snapshot, help="Interaction mode: snapshot, headless, vision"),
  record: bool = Option(False),
):
    cfg = config.load(provider=provider, model=model, mode=mode.value, record=record)
    asyncio.run(agent.run_agent_chat_session(cfg))
----

=== batch
[source,python]
----
@app.command()
def batch(
  file: Path,
  provider: str = Option(...),  # same flags as chat
  model: str = Option(...),
  mode: MCPMode = Option(MCPMode.snapshot, help="Interaction mode: snapshot, headless, vision"),
  record: bool = Option(False),
):
    cfg = config.load(provider=provider, model=model, mode=mode.value, record=record)
    lines = [l for l in file.read_text().splitlines() if l.strip()]
    asyncio.run(agent.run_agent_batch_session(cfg, lines))
    utils.wait_for_keypress()
----

== 4. Agent Core (agent.py)

Instantiate a streaming chat agent using LangGraph, wired to the Playwright-MCP tool via `langchain-mcp-adapters`.

[source,python]
----
# Example imports (adjust based on actual structure)
from langgraph.prebuilt import create_react_agent
from langchain_community.chat_models import ChatLiteLLM
from langchain_mcp_adapters.client import MultiServerMCPClient # Import the MCP client
# Assume prompts.py and config.py are correctly set up

class Agent:
    # NOTE: Initialization and running methods likely need to be async
    # due to the async nature of MultiServerMCPClient and agent invocation.
    # Simplified here for design clarity.

    def __init__(self, cfg):
        self.cfg = cfg
        # 1. Initialize LLM
        self.llm = ChatLiteLLM(provider=cfg.provider, model=cfg.model, streaming=True)

        # 2. Setup MCP Client (details abstracted for design doc)
        # The actual implementation would use 'async with MultiServerMCPClient(...)'
        # and pass the necessary server config (command, path, headless etc.)
        # For the design, we represent getting tools conceptually:
        # mcp_client = MultiServerMCPClient(cfg.mcp_server_config)
        # tools = mcp_client.get_tools() # Conceptual - actual call is async

        # MCP server config now depends on cfg.mode (passed to _setup_agent_resources internally)
        # The actual setup happens within _setup_agent_resources called by run_..._session
        llm, mcp_config, system_prompt = _setup_agent_resources(cfg)

        # Tools are loaded async within the run methods, simplified here
        tools = ["PlaceholderTool"] # Placeholder for conceptual design

        # 3. Prepare system message/prompt
        system_message = prompts.build(self.cfg.record)

        # 4. Create the LangGraph agent graph
        self.graph = create_react_agent(
            self.llm,
            tools=tools,
            state_modifier=system_message # Pass prompt via state_modifier
        )

    def _load_mcp_tools_sync(self):
        # Placeholder: In reality, this involves async setup and client.get_tools()
        # MCP configuration (like headless/vision) is now handled in _setup_agent_resources
        # based on self.cfg.mode, not directly here.
        # For the design doc, we just indicate that tools are loaded.
        # Example tool (replace with actual loaded tool)
        from langchain_mcp_adapters.tools import PlaywrightMCPTool # Example
        # The tool itself doesn't usually take headless directly, the server connection does
        return [PlaywrightMCPTool()] # Simplified representation

    def run_chat_loop(self):
        # This method is replaced by run_agent_chat_session in the actual agent.py

    def send(self, user_str):
        # Simplified send using the LangGraph agent
        print(f"Processing: {user_str}")
        # Actual invocation would be async: await self.graph.ainvoke(...)
        response = self.graph.invoke({"messages": [("user", user_str)]})
        final_message = response.get("messages", [])[-1]
        print(f"RESPONSE: {getattr(final_message, 'content', '...')}")
----

== 5. System Prompt Templates (prompts.py)

Encapsulate LLM instructions; include screenshot hint only if `--record` is set.

[source,python]
----
def build(do_screenshots: bool) -> str:
    base = """
You are a browser-automation agent.
Use the `playwright_mcp` tool to carry out user instructions one at a time.
Return only the action result or observation for each step.
"""
    if do_screenshots:
        base += """
After every successful action, call `playwright_mcp.screenshot(path=\"auto\")` to capture a PNG.
"""
    return base.strip()
----

== 6. Packaging & Entry Point

- Add in `pyproject.toml`:

[source,toml]
----
[tool.poetry.scripts]
pb-agent = "main:app"
----

- `main.py` simply invokes Typer:

[source,python]
----
from cli import app

if __name__ == "__main__":
    app()
----

== 7. Lifecycle & Shutdown Helpers (utils.py)

- Register SIGINT/SIGTERM handler for graceful application shutdown.
  (Note: Management of the external Playwright MCP server process is outside this application's scope).
- Implement `wait_for_keypress()` for batch completion.

[source,python]
----
import signal

def wait_for_keypress():
    # cross-platform getch or input
    input("Done. Press any key to exit …")

def register_shutdown(handler):
    signal.signal(signal.SIGINT, handler)
    signal.signal(signal.SIGTERM, handler)
----

== 8. Flow Summary

1. **Startup**: `main` → `cli` → `config` → instantiate `Agent`
2. **Interactive**: prompt user → chain.stream → Playwright-MCP tool → print tokens
3. **Batch**: read lines → for each line call `send` → stream output live → keypress end
4. **Shutdown**: on exit or signal, kill MCP server & exit

--

All PRD requirements are covered in discrete modules. Junior devs can follow each file stub and fill in details using the provided code snippets.