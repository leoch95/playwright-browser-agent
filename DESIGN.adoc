= Playwright Browser Agent System Design
:toc:
:toc-placement!:

== Overview
This document captures the end-to-end system design for the Playwright Browser Agent CLI demo. It maps each requirement from the PRD to modules and flows that a junior developer can implement.

== Tech Stack

- Python 3.13
- Typer (CLI framework)
- LangChain (agent orchestration)
- LiteLLM (LLM provider abstraction)
- Playwright-MCP (browser automation via Node subprocess, specifically `@playwright/mcp` NPM package)
- `langchain-mcp-adapters` (PyPI package to integrate LangChain with MCP)
- python-dotenv (env config)
- Poetry (packaging, via pyproject.toml)
- Optionally: Streamlit (future web UI), CrewAI/AutoGen (future multi-agent)
- OS: macOS, Linux, Windows WSL (with Python 3.13 and Node.js for MCP)

== 1. Project Layout
[source,text]
----
playwright-browser-agent/
├── main.py           # Typer entrypoint
├── cli.py            # Defines `chat` & `batch` commands and flags
├── config.py         # Loads .env, merges JSON, applies CLI overrides
├── agent.py          # Builds LangChain<>LiteLLM<>Playwright-MCP pipeline
├── prompts.py        # System-prompt templates
├── utils.py          # Helpers: timestamp, keypress, shutdown
└── pyproject.toml    # Dependencies + console_scripts pb-agent
----

== 2. Config Flow (config.py)

Load and merge configuration in the following precedence:

[source,python]
----
# 1. Load environment variables via python-dotenv
# 2. If --model-config JSON is provided: parse and merge into defaults
# 3. Override with explicit CLI flags: provider, model, mode, record
# 4. Fail fast if chosen PROVIDER_API_KEY is missing
----

== 3. CLI Definition (cli.py)

Use Typer to expose two commands: `chat` and `batch`.

=== chat
[source,python]
----
@app.command()
def chat(
  provider: str = Option(...),
  model: str = Option("gpt-3.5-turbo"),
  record: bool = Option(False),
  playwright_mcp_config_path: Optional[str] = Option(None),
  artifacts_dir: str = Option("artifacts")
):
    cfg = config.load(
        provider=provider,
        model=model,
        record=record
    )
    asyncio.run(agent.run_agent_chat_session(cfg))
----

=== batch
[source,python]
----
@app.command()
def batch(
  file: Path,
  provider: str = Option(...),  # same flags as chat
  model: str = Option(...),
  record: bool = Option(False),
  playwright_mcp_config_path: Optional[str] = Option(None),
  artifacts_dir: str = Option("artifacts")
):
    cfg = config.load(
        provider=provider,
        model=model,
        record=record
    )
    lines = [l for l in file.read_text().splitlines() if l.strip()]
    asyncio.run(agent.run_agent_batch_session(cfg, lines))
    utils.wait_for_keypress()
----

== 4. Agent Core (agent.py)

Instantiate a streaming chat agent using LangGraph, wired to the Playwright-MCP tool via `langchain-mcp-adapters`.

[source,python]
----
# Example imports (adjust based on actual structure)
from langgraph.prebuilt import create_react_agent
from langchain_community.chat_models import ChatLiteLLM
from langchain_mcp_adapters.client import MultiServerMCPClient # Import the MCP client
# Assume prompts.py and config.py are correctly set up
from .prompts import build_system_prompt # Use actual import

class Agent:
    # NOTE: Initialization and running methods likely need to be async
    # due to the async nature of MultiServerMCPClient and agent invocation.
    # Simplified here for design clarity.

    def __init__(self, cfg):
        self.cfg = cfg
        # 1. Initialize LLM
        self.llm = ChatLiteLLM(provider=cfg.provider, model=cfg.model, streaming=True)

        # 2. Setup MCP Client and get tools conceptually
        # The actual setup (including passing mode/headless to configure MCP server)
        # happens within _setup_agent_resources called by run_..._session
        llm, mcp_config, system_prompt = _setup_agent_resources(cfg)

        # Tools are loaded async within the run methods, simplified here
        tools = ["PlaceholderTool"] # Placeholder for conceptual design

        # 3. Prepare system message/prompt using the actual function
        # system_message is built inside _setup_agent_resources now, using build_system_prompt
        # system_message = prompts.build_system_prompt(
        #     mode=self.cfg.mode,
        #     headless=self.cfg.headless,
        #     record_screenshots=self.cfg.record
        # )

        # 4. Create the LangGraph agent graph
        self.graph = create_react_agent(
            self.llm,
            tools=tools,
            # System prompt is typically passed within the messages input dictionary
            # state_modifier=system_message # Pass prompt via state_modifier - Not typical for create_react_agent
        )

    # Remove _load_mcp_tools_sync and run_chat_loop placeholders
    # as the logic is now in async run_agent_*_session functions

    def send(self, user_str):
        # Simplified send using the LangGraph agent - Primarily for batch concept
        print(f"Processing: {user_str}")
        # Actual invocation would be async: await self.graph.ainvoke or astream_events
        # System prompt is passed in messages list in the actual implementation
        response = self.graph.invoke({"messages": [
            ("system", "Placeholder System Prompt"), # System prompt passed here
            ("user", user_str)
        ]})
        final_message = response.get("messages", [])[-1]
        print(f"RESPONSE: {getattr(final_message, 'content', '...')}")

    # Removed original send method as it was synchronous and not using astream_events
    # def send(self, user_str):
    #     # Simplified send using the LangGraph agent - Primarily for batch concept
    #     print(f"Processing: {user_str}")
    #     # Actual invocation would be async: await self.graph.ainvoke or astream_events
    #     # System prompt is passed in messages list in the actual implementation
    #     response = self.graph.invoke({"messages": [
    #         ("system", "Placeholder System Prompt"), # System prompt passed here
    #         ("user", user_str)
    #     ]})
    #     final_message = response.get("messages", [])[-1]
    #     print(f"RESPONSE: {getattr(final_message, 'content', '...')}")

    # The actual method for sending user input and getting a response
    # would involve invoking the graph, potentially with streaming.
    # Example for conceptual clarity:
    async def send_message(self, user_input: str, system_prompt_str: str):
        # In a real scenario, messages would accumulate in state
        messages = [
            ("system", system_prompt_str),
            ("user", user_input)
        ]
        # Example of how one might stream events from the graph
        async for event in self.graph.astream_events(
            {"messages": messages}, version="v1"
        ):
            # Process different types of events (on_chat_model_stream, on_tool_end, etc.)
            # This part needs to be fleshed out based on LangGraph's event streaming API
            # For now, just printing the event type
            print(f"Event: {event['event']}")
            if event['event'] == "on_chat_model_stream":
                chunk = event["data"]["chunk"]
                if hasattr(chunk, 'content'):
                    print(chunk.content, end="", flush=True)
            # Add more event handling as needed

== 5. System Prompt Templates (prompts.py)

Encapsulate LLM instructions; include screenshot hint only if `--record` is set.

[source,python]
----
def build_system_prompt(record_screenshots: bool = False, headless_status: bool = False, artifacts_dir: str = "artifacts") -> str:
    # Base template with placeholders
    base_template = """
    You are a web browsing agent...
    Your goal is to assist the user with web navigation and interaction tasks.
    You can use the provided tools to interact with the web page.
    {headless_status_detail}
    Current Configuration:
    - Headless: {is_headless}
    - Recording Screenshots: {is_recording}
    """

    headless_status_detail_str = "You are operating in a headless browser environment. You will not see the browser GUI. Rely on the observation data provided." if headless_status else "You are operating in a browser with a visible GUI."

    prompt = base_template.format(
        headless_status_detail=headless_status_detail_str,
        is_headless=str(headless_status),
        is_recording=str(record_screenshots)
    )

    if record_screenshots:
        # Note: The actual path construction including timestamp and step number
        # will be handled by the agent/tool when a screenshot is taken.
        # The prompt only needs to inform the LLM that recording is active
        # and where screenshots are generally stored.
        SCREENSHOT_INSTRUCTIONS = f"""
        Screenshots are being recorded after your actions.
        They will be saved in a timestamped subfolder within: {artifacts_dir}
        Example path: {artifacts_dir}/YYYYMMDD_HHMMSS_ffffff/step_N.png
        When asked to take a screenshot, use the appropriate tool. You don't need to specify the filename.
        """
        prompt += "\n\n" + SCREENSHOT_INSTRUCTIONS

    return prompt.strip()
----

== 6. Packaging & Entry Point

- Add in `pyproject.toml`:

[source,toml]
----
[tool.poetry.scripts]
pb-agent = "main:app"
----

- `main.py` simply invokes Typer:

[source,python]
----
from cli import app

if __name__ == "__main__":
    app()
----

== 7. Lifecycle & Shutdown Helpers (utils.py)

- Register SIGINT/SIGTERM handler for graceful application shutdown.
  (Note: Management of the external Playwright MCP server process is outside this application's scope).
- Implement `wait_for_keypress()` for batch completion.

[source,python]
----
import signal

def wait_for_keypress():
    # cross-platform getch or input
    input("Done. Press any key to exit …")

def register_shutdown(handler):
    signal.signal(signal.SIGINT, handler)
    signal.signal(signal.SIGTERM, handler)
----

== 8. Flow Summary

1. **Startup**: `main` → `cli` → `config` → instantiate `Agent`
2. **Interactive**: prompt user → chain.stream → Playwright-MCP tool → print tokens
3. **Batch**: read lines → for each line call `send` → stream output live → keypress end
4. **Shutdown**: on exit or signal, kill MCP server & exit

--

All PRD requirements are covered in discrete modules. Junior devs can follow each file stub and fill in details using the provided code snippets.